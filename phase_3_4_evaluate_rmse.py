# phase_3_4_evaluate_rmse.py - Evaluate RMSE on Validation Set
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import os
from datetime import datetime
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

load_dotenv()

def setup_database_connection():
    """Setup database connection"""
    try:
        pg_user = os.getenv('POSTGRES_USER', 'postgres')
        pg_password = os.getenv('POSTGRES_PASSWORD', 'password')
        pg_host = os.getenv('POSTGRES_HOST', 'localhost')
        pg_port = os.getenv('POSTGRES_PORT', '5432')
        pg_database = os.getenv('POSTGRES_DATABASE', 'smart_dispatch')
        
        pg_connection_string = f"postgresql://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_database}"
        engine = create_engine(pg_connection_string)
        
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        print("âœ… PostgreSQL connection successful")
        return engine, "postgresql"
        
    except Exception as e:
        print(f"âš ï¸ PostgreSQL not available, using SQLite")
        sqlite_path = "market_data.db"
        sqlite_connection_string = f"sqlite:///{sqlite_path}"
        engine = create_engine(sqlite_connection_string)
        print(f"âœ… SQLite connection successful: {sqlite_path}")
        return engine, "sqlite"

def load_and_prepare_data(engine):
    """Load and prepare data for model training"""
    print(f"ðŸ”„ Loading and preparing data...")
    
    # Load feature matrix
    query = "SELECT * FROM features ORDER BY window_id"
    df_features = pd.read_sql(query, engine)
    df_features['target_time'] = pd.to_datetime(df_features['target_time'])
    
    # Define feature columns
    exclude_cols = ['window_id', 'target_time', 'target_price', 'price_sequence_json']
    feature_cols = [col for col in df_features.columns if col not in exclude_cols]
    
    # Extract features and targets
    X = df_features[feature_cols].values
    y = df_features['target_price'].values
    
    # Extract price sequences
    price_sequences = []
    for seq_json in df_features['price_sequence_json']:
        seq = json.loads(seq_json)
        price_sequences.append(seq)
    price_sequences = np.array(price_sequences)
    
    # Train/test split
    X_train, X_test, y_train, y_test, seq_train, seq_test = train_test_split(
        X, y, price_sequences, test_size=0.2, random_state=42
    )
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"âœ… Data prepared: {len(X_train_scaled)} train, {len(X_test_scaled)} test samples")
    
    return X_train_scaled, X_test_scaled, y_train, y_test, seq_train, seq_test, scaler, feature_cols

class PowerMarketDataset(Dataset):
    """PyTorch Dataset for power market data"""
    
    def __init__(self, X, y, sequences):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
        self.sequences = torch.FloatTensor(sequences)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.sequences[idx], self.y[idx]

class PowerMarketLSTM(nn.Module):
    """LSTM Model Architecture for Power Market Forecasting"""
    
    def __init__(self, n_features, sequence_length=24, lstm_hidden_size=64, 
                 lstm_num_layers=2, dense_hidden_size=32, dropout_rate=0.2):
        super(PowerMarketLSTM, self).__init__()
        
        self.n_features = n_features
        self.sequence_length = sequence_length
        self.lstm_hidden_size = lstm_hidden_size
        self.lstm_num_layers = lstm_num_layers
        self.dense_hidden_size = dense_hidden_size
        self.dropout_rate = dropout_rate
        
        # LSTM branch for processing price sequences
        self.lstm = nn.LSTM(
            input_size=1,
            hidden_size=lstm_hidden_size,
            num_layers=lstm_num_layers,
            batch_first=True,
            dropout=dropout_rate if lstm_num_layers > 1 else 0
        )
        
        # Dense branch for processing engineered features
        self.feature_layers = nn.Sequential(
            nn.Linear(n_features, dense_hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(dense_hidden_size * 2, dense_hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # Combined prediction layers
        combined_input_size = lstm_hidden_size + dense_hidden_size
        self.prediction_layers = nn.Sequential(
            nn.Linear(combined_input_size, dense_hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(dense_hidden_size, dense_hidden_size // 2),
            nn.ReLU(),
            nn.Linear(dense_hidden_size // 2, 1)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize model weights"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                if 'lstm' in name:
                    nn.init.xavier_uniform_(param)
                else:
                    nn.init.kaiming_uniform_(param, nonlinearity='relu')
            elif 'bias' in name:
                nn.init.constant_(param, 0)
    
    def forward(self, features, sequences):
        """Forward pass through the hybrid LSTM architecture"""
        batch_size = features.size(0)
        
        # LSTM branch: Process price sequences
        lstm_input = sequences.unsqueeze(-1)
        lstm_output, (hidden, cell) = self.lstm(lstm_input)
        lstm_features = hidden[-1]
        
        # Dense branch: Process engineered features
        dense_features = self.feature_layers(features)
        
        # Combine LSTM and dense features
        combined_features = torch.cat([lstm_features, dense_features], dim=1)
        
        # Final prediction
        predictions = self.prediction_layers(combined_features)
        
        return predictions.squeeze(-1)

def train_model(model, train_loader, test_loader, epochs=5, learning_rate=0.001):
    """Train the model and return trained model"""
    print(f"ðŸ”„ Training model for {epochs} epochs...")
    
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)
    
    for epoch in range(1, epochs + 1):
        # Training phase
        model.train()
        train_loss = 0.0
        train_samples = 0
        
        for batch_idx, (features, sequences, targets) in enumerate(train_loader):
            optimizer.zero_grad()
            predictions = model(features, sequences)
            loss = criterion(predictions, targets)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item() * len(targets)
            train_samples += len(targets)
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_samples = 0
        
        with torch.no_grad():
            for features, sequences, targets in test_loader:
                predictions = model(features, sequences)
                loss = criterion(predictions, targets)
                val_loss += loss.item() * len(targets)
                val_samples += len(targets)
        
        avg_train_loss = train_loss / train_samples
        avg_val_loss = val_loss / val_samples
        scheduler.step(avg_val_loss)
        
        if epoch == 1 or epoch == epochs:
            print(f"   Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")
    
    print(f"âœ… Model training completed")
    return model

def evaluate_rmse_on_validation_set(model, test_loader, y_train):
    """
    Phase 3.4: Evaluate RMSE on Validation Set
    Test Case: RMSE < naive last-hour prediction baseline
    """
    print(f"ðŸ”„ Phase 3.4: Evaluating RMSE on validation set...")
    
    # Get model predictions on validation set
    model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for features, sequences, targets in test_loader:
            predictions = model(features, sequences)
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    
    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)
    
    # Calculate RMSE and other metrics
    model_rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))
    model_mae = mean_absolute_error(all_targets, all_predictions)
    model_r2 = r2_score(all_targets, all_predictions)
    
    # Calculate baseline predictions
    # Naive baseline 1: Mean of training data
    mean_baseline = np.mean(y_train)
    mean_predictions = np.full_like(all_targets, mean_baseline)
    mean_rmse = np.sqrt(mean_squared_error(all_targets, mean_predictions))
    mean_mae = mean_absolute_error(all_targets, mean_predictions)
    
    # Naive baseline 2: Last-hour prediction (use last known value)
    # For validation, we'll use the mean of the last few training samples as "last hour"
    last_hour_baseline = np.mean(y_train[-3:])  # Average of last 3 training samples
    last_hour_predictions = np.full_like(all_targets, last_hour_baseline)
    last_hour_rmse = np.sqrt(mean_squared_error(all_targets, last_hour_predictions))
    last_hour_mae = mean_absolute_error(all_targets, last_hour_predictions)
    
    # Test Case: RMSE < naive last-hour prediction baseline
    rmse_better_than_baseline = model_rmse < last_hour_rmse
    
    print(f"\nðŸ“Š RMSE Evaluation Results:")
    print(f"   Model RMSE: ${model_rmse:.2f}")
    print(f"   Model MAE:  ${model_mae:.2f}")
    print(f"   Model RÂ²:   {model_r2:.4f}")
    
    print(f"\nðŸ“ˆ Baseline Comparisons:")
    print(f"   Mean Baseline RMSE: ${mean_rmse:.2f}")
    print(f"   Last-Hour Baseline RMSE: ${last_hour_rmse:.2f}")
    
    print(f"\nðŸ§ª Test Case - RMSE < naive last-hour prediction baseline:")
    print(f"   Model RMSE: ${model_rmse:.2f}")
    print(f"   Last-hour baseline RMSE: ${last_hour_rmse:.2f}")
    print(f"   RMSE improvement: {((last_hour_rmse - model_rmse) / last_hour_rmse * 100):.1f}%")
    print(f"   Result: {'âœ… PASSED' if rmse_better_than_baseline else 'âŒ FAILED'}")
    
    # Performance analysis
    print(f"\nðŸŽ¯ Detailed Performance Analysis:")
    
    # Error distribution
    errors = all_predictions - all_targets
    error_std = np.std(errors)
    error_mean = np.mean(errors)
    
    print(f"   Prediction errors:")
    print(f"     Mean error (bias): ${error_mean:.2f}")
    print(f"     Error std dev: ${error_std:.2f}")
    print(f"     Max error: ${np.max(np.abs(errors)):.2f}")
    
    # Percentage errors
    percentage_errors = np.abs(errors / all_targets * 100)
    mean_percentage_error = np.mean(percentage_errors)
    
    print(f"   Percentage errors:")
    print(f"     Mean absolute percentage error: {mean_percentage_error:.1f}%")
    print(f"     Median absolute percentage error: {np.median(percentage_errors):.1f}%")
    
    # Prediction vs actual comparison
    print(f"\nðŸ“‹ Predictions vs Actual (All validation samples):")
    print(f"{'#':<3} {'Actual':<10} {'Predicted':<10} {'Error':<10} {'Error %':<8}")
    print(f"{'-'*3} {'-'*10} {'-'*10} {'-'*10} {'-'*8}")
    
    for i in range(len(all_targets)):
        actual = all_targets[i]
        predicted = all_predictions[i]
        error = predicted - actual
        error_pct = (error / actual * 100) if actual != 0 else 0
        
        print(f"{i+1:<3} ${actual:<9.2f} ${predicted:<9.2f} ${error:<9.2f} {error_pct:<7.1f}%")
    
    # Model performance summary
    better_than_mean = model_rmse < mean_rmse
    better_than_last_hour = model_rmse < last_hour_rmse
    
    print(f"\nâœ… Phase 3.4 Evaluation Summary:")
    print(f"   RMSE Validation: ${model_rmse:.2f}")
    print(f"   Better than mean baseline: {'âœ…' if better_than_mean else 'âŒ'}")
    print(f"   Better than last-hour baseline: {'âœ…' if better_than_last_hour else 'âŒ'}")
    print(f"   Model learning: {'âœ… Confirmed' if better_than_mean or better_than_last_hour else 'âš ï¸ Needs improvement'}")
    
    evaluation_results = {
        'model_rmse': model_rmse,
        'model_mae': model_mae,
        'model_r2': model_r2,
        'mean_baseline_rmse': mean_rmse,
        'last_hour_baseline_rmse': last_hour_rmse,
        'rmse_better_than_baseline': rmse_better_than_baseline,
        'predictions': all_predictions,
        'targets': all_targets,
        'errors': errors
    }
    
    return evaluation_results

def main():
    """Execute Phase 3.4 workflow"""
    print("ðŸš€ Phase 3.4: Evaluate RMSE on Validation Set")
    
    try:
        # Step 1: Setup database and load data
        engine, db_type = setup_database_connection()
        X_train, X_test, y_train, y_test, seq_train, seq_test, scaler, feature_cols = load_and_prepare_data(engine)
        
        # Step 2: Create and train model
        n_features = len(feature_cols)
        
        model = PowerMarketLSTM(
            n_features=n_features,
            sequence_length=24,
            lstm_hidden_size=64,
            lstm_num_layers=2,
            dense_hidden_size=32,
            dropout_rate=0.2
        )
        
        # Create datasets and loaders
        train_dataset = PowerMarketDataset(X_train, y_train, seq_train)
        test_dataset = PowerMarketDataset(X_test, y_test, seq_test)
        
        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
        
        # Train model
        trained_model = train_model(model, train_loader, test_loader, epochs=5)
        
        # Step 3: Evaluate RMSE on validation set (Phase 3.4)
        evaluation_results = evaluate_rmse_on_validation_set(trained_model, test_loader, y_train)
        
        print(f"\nâœ… Phase 3.4 COMPLETE: RMSE evaluation completed")
        print(f"ðŸ“Š Final RMSE: ${evaluation_results['model_rmse']:.2f}")
        print(f"ðŸŽ¯ Baseline comparison: {'âœ… PASSED' if evaluation_results['rmse_better_than_baseline'] else 'Needs optimization'}")
        print(f"ðŸ”„ Next: Phase 3.5 - Save Model to File")
        
        return trained_model, evaluation_results, scaler, feature_cols
        
    except Exception as e:
        print(f"âŒ Phase 3.4 failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

if __name__ == "__main__":
    trained_model, evaluation_results, scaler, feature_cols = main()