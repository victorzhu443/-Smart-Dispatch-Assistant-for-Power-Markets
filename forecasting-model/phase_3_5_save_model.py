# phase_3_5_save_model.py - Save Model to File model.pt (Fixed for PyTorch 2.7+)
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import os
import pickle
from datetime import datetime
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error

load_dotenv()

def setup_database_connection():
    """Setup database connection"""
    try:
        pg_user = os.getenv('POSTGRES_USER', 'postgres')
        pg_password = os.getenv('POSTGRES_PASSWORD', 'password')
        pg_host = os.getenv('POSTGRES_HOST', 'localhost')
        pg_port = os.getenv('POSTGRES_PORT', '5432')
        pg_database = os.getenv('POSTGRES_DATABASE', 'smart_dispatch')
        
        pg_connection_string = f"postgresql://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_database}"
        engine = create_engine(pg_connection_string)
        
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        print("‚úÖ PostgreSQL connection successful")
        return engine, "postgresql"
        
    except Exception as e:
        print(f"‚ö†Ô∏è PostgreSQL not available, using SQLite")
        sqlite_path = "market_data.db"
        sqlite_connection_string = f"sqlite:///{sqlite_path}"
        engine = create_engine(sqlite_connection_string)
        print(f"‚úÖ SQLite connection successful: {sqlite_path}")
        return engine, "sqlite"

def load_and_prepare_data(engine):
    """Load and prepare data for model training"""
    print(f"üîÑ Loading and preparing data...")
    
    # Load feature matrix
    query = "SELECT * FROM features ORDER BY window_id"
    df_features = pd.read_sql(query, engine)
    df_features['target_time'] = pd.to_datetime(df_features['target_time'])
    
    # Define feature columns
    exclude_cols = ['window_id', 'target_time', 'target_price', 'price_sequence_json']
    feature_cols = [col for col in df_features.columns if col not in exclude_cols]
    
    # Extract features and targets
    X = df_features[feature_cols].values
    y = df_features['target_price'].values
    
    # Extract price sequences
    price_sequences = []
    for seq_json in df_features['price_sequence_json']:
        seq = json.loads(seq_json)
        price_sequences.append(seq)
    price_sequences = np.array(price_sequences)
    
    # Train/test split
    X_train, X_test, y_train, y_test, seq_train, seq_test = train_test_split(
        X, y, price_sequences, test_size=0.2, random_state=42
    )
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"‚úÖ Data prepared: {len(X_train_scaled)} train, {len(X_test_scaled)} test samples")
    
    return X_train_scaled, X_test_scaled, y_train, y_test, seq_train, seq_test, scaler, feature_cols

class PowerMarketDataset(Dataset):
    """PyTorch Dataset for power market data"""
    
    def __init__(self, X, y, sequences):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
        self.sequences = torch.FloatTensor(sequences)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.sequences[idx], self.y[idx]

class PowerMarketLSTM(nn.Module):
    """LSTM Model Architecture for Power Market Forecasting"""
    
    def __init__(self, n_features, sequence_length=24, lstm_hidden_size=64, 
                 lstm_num_layers=2, dense_hidden_size=32, dropout_rate=0.2):
        super(PowerMarketLSTM, self).__init__()
        
        self.n_features = n_features
        self.sequence_length = sequence_length
        self.lstm_hidden_size = lstm_hidden_size
        self.lstm_num_layers = lstm_num_layers
        self.dense_hidden_size = dense_hidden_size
        self.dropout_rate = dropout_rate
        
        # LSTM branch for processing price sequences
        self.lstm = nn.LSTM(
            input_size=1,
            hidden_size=lstm_hidden_size,
            num_layers=lstm_num_layers,
            batch_first=True,
            dropout=dropout_rate if lstm_num_layers > 1 else 0
        )
        
        # Dense branch for processing engineered features
        self.feature_layers = nn.Sequential(
            nn.Linear(n_features, dense_hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(dense_hidden_size * 2, dense_hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # Combined prediction layers
        combined_input_size = lstm_hidden_size + dense_hidden_size
        self.prediction_layers = nn.Sequential(
            nn.Linear(combined_input_size, dense_hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(dense_hidden_size, dense_hidden_size // 2),
            nn.ReLU(),
            nn.Linear(dense_hidden_size // 2, 1)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize model weights"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                if 'lstm' in name:
                    nn.init.xavier_uniform_(param)
                else:
                    nn.init.kaiming_uniform_(param, nonlinearity='relu')
            elif 'bias' in name:
                nn.init.constant_(param, 0)
    
    def forward(self, features, sequences):
        """Forward pass through the hybrid LSTM architecture"""
        batch_size = features.size(0)
        
        # LSTM branch: Process price sequences
        lstm_input = sequences.unsqueeze(-1)
        lstm_output, (hidden, cell) = self.lstm(lstm_input)
        lstm_features = hidden[-1]
        
        # Dense branch: Process engineered features
        dense_features = self.feature_layers(features)
        
        # Combine LSTM and dense features
        combined_features = torch.cat([lstm_features, dense_features], dim=1)
        
        # Final prediction
        predictions = self.prediction_layers(combined_features)
        
        return predictions.squeeze(-1)

def train_model(model, train_loader, test_loader, epochs=5, learning_rate=0.001):
    """Train the model and return trained model with training history"""
    print(f"üîÑ Training model for {epochs} epochs...")
    
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)
    
    training_history = {
        'train_losses': [],
        'val_losses': [],
        'epochs': epochs,
        'learning_rate': learning_rate
    }
    
    for epoch in range(1, epochs + 1):
        # Training phase
        model.train()
        train_loss = 0.0
        train_samples = 0
        
        for batch_idx, (features, sequences, targets) in enumerate(train_loader):
            optimizer.zero_grad()
            predictions = model(features, sequences)
            loss = criterion(predictions, targets)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item() * len(targets)
            train_samples += len(targets)
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_samples = 0
        
        with torch.no_grad():
            for features, sequences, targets in test_loader:
                predictions = model(features, sequences)
                loss = criterion(predictions, targets)
                val_loss += loss.item() * len(targets)
                val_samples += len(targets)
        
        avg_train_loss = train_loss / train_samples
        avg_val_loss = val_loss / val_samples
        
        training_history['train_losses'].append(avg_train_loss)
        training_history['val_losses'].append(avg_val_loss)
        
        scheduler.step(avg_val_loss)
        
        if epoch == 1 or epoch == epochs:
            print(f"   Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")
    
    print(f"‚úÖ Model training completed")
    return model, training_history

def save_model_to_file(model, scaler, feature_cols, training_history, filename="model.pt"):
    """
    Phase 3.5: Save Model to File model.pt
    Test Case: File exists and reloads without error
    """
    print(f"üîÑ Phase 3.5: Saving model to file '{filename}'...")
    
    # Prepare model state for saving
    model_state = {
        'model_state_dict': model.state_dict(),
        'model_config': {
            'n_features': model.n_features,
            'sequence_length': model.sequence_length,
            'lstm_hidden_size': model.lstm_hidden_size,
            'lstm_num_layers': model.lstm_num_layers,
            'dense_hidden_size': model.dense_hidden_size,
            'dropout_rate': model.dropout_rate
        },
        'scaler': scaler,
        'feature_cols': feature_cols,
        'training_history': training_history,
        'model_class': 'PowerMarketLSTM',
        'save_timestamp': datetime.now().isoformat(),
        'pytorch_version': torch.__version__,
        'model_parameters': sum(p.numel() for p in model.parameters()),
        'model_size_mb': sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)
    }
    
    try:
        # Save model state
        torch.save(model_state, filename)
        
        # Get file info
        file_size = os.path.getsize(filename)
        file_size_mb = file_size / (1024 * 1024)
        
        print(f"‚úÖ Model saved to '{filename}'")
        print(f"   File size: {file_size_mb:.2f} MB")
        print(f"   Parameters: {model_state['model_parameters']:,}")
        print(f"   PyTorch version: {model_state['pytorch_version']}")
        print(f"   Save timestamp: {model_state['save_timestamp']}")
        
        # Test Case: File exists and reloads without error
        print(f"\nüß™ Test Case - File exists and reloads without error:")
        
        # Check if file exists
        file_exists = os.path.exists(filename)
        print(f"   File exists: {'‚úÖ' if file_exists else '‚ùå'}")
        
        if file_exists:
            try:
                # Try to reload the model (PyTorch 2.7+ compatibility)
                loaded_state = torch.load(filename, map_location='cpu', weights_only=False)
                
                # Verify essential components
                has_model_state = 'model_state_dict' in loaded_state
                has_config = 'model_config' in loaded_state
                has_scaler = 'scaler' in loaded_state
                has_features = 'feature_cols' in loaded_state
                
                print(f"   Model state dict: {'‚úÖ' if has_model_state else '‚ùå'}")
                print(f"   Model config: {'‚úÖ' if has_config else '‚ùå'}")
                print(f"   Scaler: {'‚úÖ' if has_scaler else '‚ùå'}")
                print(f"   Feature columns: {'‚úÖ' if has_features else '‚ùå'}")
                
                # Try to recreate model
                if has_config:
                    config = loaded_state['model_config']
                    test_model = PowerMarketLSTM(**config)
                    test_model.load_state_dict(loaded_state['model_state_dict'])
                    test_model.eval()
                    
                    print(f"   Model recreation: ‚úÖ")
                    
                    # Test forward pass with dummy data
                    dummy_features = torch.randn(1, config['n_features'])
                    dummy_sequences = torch.randn(1, config['sequence_length'])
                    
                    with torch.no_grad():
                        test_prediction = test_model(dummy_features, dummy_sequences)
                    
                    print(f"   Forward pass test: ‚úÖ")
                    print(f"   Test prediction shape: {test_prediction.shape}")
                    print(f"   Test prediction value: ${test_prediction.item():.2f}")
                    
                    reload_success = True
                else:
                    reload_success = False
                    print(f"   Model recreation: ‚ùå (missing config)")
                
            except Exception as e:
                reload_success = False
                print(f"   Reload error: ‚ùå {str(e)[:150]}...")
        else:
            reload_success = False
        
        test_passed = file_exists and reload_success
        print(f"   Overall result: {'‚úÖ PASSED' if test_passed else '‚ùå FAILED'}")
        
        # Show saved model contents
        if file_exists and reload_success:
            print(f"\nüìã Saved Model Contents:")
            print(f"   Model class: {loaded_state.get('model_class', 'Unknown')}")
            print(f"   Feature columns ({len(loaded_state.get('feature_cols', []))}): {loaded_state.get('feature_cols', [])[:5]}...")
            print(f"   Training history keys: {list(loaded_state.get('training_history', {}).keys())}")
            
            if 'training_history' in loaded_state:
                history = loaded_state['training_history']
                if 'train_losses' in history and len(history['train_losses']) > 0:
                    initial_loss = history['train_losses'][0]
                    final_loss = history['train_losses'][-1]
                    loss_reduction = ((initial_loss - final_loss) / initial_loss * 100)
                    print(f"   Training progress: {initial_loss:.2f} ‚Üí {final_loss:.2f} ({loss_reduction:.1f}% reduction)")
        
        return test_passed, filename, file_size_mb
        
    except Exception as e:
        print(f"‚ùå Failed to save model: {e}")
        return False, filename, 0

def create_model_usage_example(filename="model.pt"):
    """Create example code for loading and using the saved model"""
    
    usage_code = f'''# Example: How to load and use the saved model
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load the saved model (PyTorch 2.7+ compatibility)
model_state = torch.load('{filename}', map_location='cpu', weights_only=False)

# Recreate the model
from your_model_file import PowerMarketLSTM  # Import your model class
config = model_state['model_config']
model = PowerMarketLSTM(**config)
model.load_state_dict(model_state['model_state_dict'])
model.eval()

# Get the scaler and feature columns
scaler = model_state['scaler']
feature_cols = model_state['feature_cols']

# Example prediction
def predict_price(new_features, new_sequence):
    """Make a price prediction with new data"""
    # Scale features
    new_features_scaled = scaler.transform(new_features.reshape(1, -1))
    
    # Convert to tensors
    features_tensor = torch.FloatTensor(new_features_scaled)
    sequence_tensor = torch.FloatTensor(new_sequence).unsqueeze(0)
    
    # Make prediction
    with torch.no_grad():
        prediction = model(features_tensor, sequence_tensor)
    
    return prediction.item()

# Example usage:
# new_features = np.array([...])  # 19 engineered features
# new_sequence = np.array([...])  # 24-hour price sequence
# predicted_price = predict_price(new_features, new_sequence)
# print(f"Predicted next-hour price: ${{predicted_price:.2f}}")

# Model information
print(f"Model loaded successfully!")
print(f"Features: {{len(feature_cols)}}")
print(f"Feature names: {{feature_cols}}")
print(f"Training history: {{list(model_state['training_history'].keys())}}")
'''
    
    # Save usage example to file
    with open("model_usage_example.py", "w") as f:
        f.write(usage_code)
    
    print(f"üìù Created model usage example: model_usage_example.py")
    return usage_code

def main():
    """Execute Phase 3.5 workflow"""
    print("üöÄ Phase 3.5: Save Model to File model.pt")
    
    try:
        # Step 1: Setup database and load data
        engine, db_type = setup_database_connection()
        X_train, X_test, y_train, y_test, seq_train, seq_test, scaler, feature_cols = load_and_prepare_data(engine)
        
        # Step 2: Create and train model
        n_features = len(feature_cols)
        
        model = PowerMarketLSTM(
            n_features=n_features,
            sequence_length=24,
            lstm_hidden_size=64,
            lstm_num_layers=2,
            dense_hidden_size=32,
            dropout_rate=0.2
        )
        
        # Create datasets and loaders
        train_dataset = PowerMarketDataset(X_train, y_train, seq_train)
        test_dataset = PowerMarketDataset(X_test, y_test, seq_test)
        
        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
        
        # Train model
        trained_model, training_history = train_model(model, train_loader, test_loader, epochs=5)
        
        # Step 3: Save model to file (Phase 3.5)
        test_passed, filename, file_size_mb = save_model_to_file(
            trained_model, scaler, feature_cols, training_history, filename="model.pt"
        )
        
        # Step 4: Create usage example
        usage_code = create_model_usage_example(filename)
        
        if test_passed:
            print(f"\n‚úÖ Phase 3.5 COMPLETE: Model successfully saved to '{filename}'")
            print(f"üìÅ File size: {file_size_mb:.2f} MB")
            print(f"üîß Model ready for deployment and inference")
            print(f"üìù Usage example created: model_usage_example.py")
            
            print(f"\nüéØ Phase 3 Summary - Forecasting Model:")
            print(f"   ‚úÖ 3.1: Load Feature Matrix - 51 samples loaded")
            print(f"   ‚úÖ 3.2: Define LSTM Architecture - 57,441 parameters")
            print(f"   ‚úÖ 3.3: Train Model (5 epochs) - Loss reduction achieved")
            print(f"   ‚úÖ 3.4: Evaluate RMSE - Baseline comparison completed")
            print(f"   ‚úÖ 3.5: Save Model to File - model.pt created and verified")
            
            print(f"\nüöÄ Ready for Phase 4: LLM + Retrieval Integration (RAG)")
        else:
            print(f"\n‚ö†Ô∏è Phase 3.5 completed with issues: Model saved but verification needs attention")
            print(f"üí° The model file exists and can likely be loaded with weights_only=False")
        
        return trained_model, filename, test_passed
        
    except Exception as e:
        print(f"‚ùå Phase 3.5 failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None, False

if __name__ == "__main__":
    trained_model, filename, success = main()
